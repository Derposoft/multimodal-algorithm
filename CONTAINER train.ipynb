{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install pyarrow\n",
    "!pip install pandarallel\n",
    "!pip install pandas\n",
    "!pip install mxnet\n",
    "!pip install autogluon\n",
    "!pip install gluoncv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os, time\n",
    "import autogluon as ag\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, init, autograd\n",
    "from mxnet.gluon import nn\n",
    "import pickle\n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(os.cpu_count())\n",
    "pandarallel.initialize()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "mx.random.seed(127)\n",
    "#gpus = mx.test_utils.list_gpus()\n",
    "contexts = [mx.cpu()] #[mx.gpu(i) for i in gpus] if len(gpus) > 0 else [mx.cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data / parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './Data/container_formatted/train.csv'\n",
    "model_path = './opt/ml/model/'\n",
    "n_hours = 0\n",
    "n_mins = 15\n",
    "time_limits = 60 * 60 * n_hours + 60 * n_mins # n_hours + n_mins\n",
    "eval_metric = 'accuracy'\n",
    "id_column = 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text/numeric data\n",
    "dataset = pd.read_csv(train_dataset_path)\n",
    "print('data read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create category mapping (to define non-clashing dummy category)\n",
    "target_column = \"label\"\n",
    "cats = dataset[target_column].unique()\n",
    "numcats = len(cats)\n",
    "intcats = list(range(numcats))\n",
    "catmap = dict(zip(cats, intcats))\n",
    "invcatmap = dict(zip(intcats, cats))\n",
    "\n",
    "# map labels to new categories\n",
    "labels = dataset[target_column]\n",
    "mappedlabels = pd.Series([catmap[x] for x in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data clean / suggested label generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, base64\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from io import StringIO, BytesIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "def clean_text(data, labelled=False):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # scrub possible html\n",
    "    # stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "    def strip_tags(html):\n",
    "        s = MLStripper()\n",
    "        s.feed(html)\n",
    "        return s.get_data()\n",
    "    # lowercases and removes special characters\n",
    "    def clean_val(val):\n",
    "        text = val\n",
    "        text = strip_tags(text)\n",
    "        if type(val) == str:\n",
    "            text = text.lower().strip()\n",
    "            text = re.compile(r'[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            words = [w for w in text.split(\" \") if not w in stop_words]\n",
    "            text = \" \".join([lemmatizer.lemmatize(w) for w in words])\n",
    "        return text\n",
    "    # get text cols\n",
    "    text_cols = [x for x in data.columns if x.endswith('_text')]\n",
    "    if len(text_cols) == 0:\n",
    "        print('no text columns found.')\n",
    "        return pd.DataFrame(), None\n",
    "    print('text columns found:', text_cols)\n",
    "    text_data = data[text_cols].copy()\n",
    "    # lazy impute text\n",
    "    text_data = text_data.fillna('')\n",
    "    # clean text cols\n",
    "    for col in text_cols:\n",
    "        print('text cleaning:', col)\n",
    "        text_data[col] = text_data[col].parallel_apply(clean_val)\n",
    "    text_label = None\n",
    "    if labelled:\n",
    "        print('working on labels...')\n",
    "        text_label = mappedlabels.copy()\n",
    "        map2bad = text_data[text_cols[0]] == ''\n",
    "        for col in text_cols[1:]:\n",
    "            map2bad = np.logical_and(map2bad, text_data[col] == '')\n",
    "        text_label[map2bad] = numcats\n",
    "    print(\"done transforming text data.\")\n",
    "    return text_data, text_label\n",
    "def clean_num(data, labelled=False):\n",
    "    # get num cols\n",
    "    num_cols = [x for x in data.columns if x.endswith('_num')]\n",
    "    if len(num_cols) == 0:\n",
    "        print('no numeric columns found.')\n",
    "        return pd.DataFrame(), None\n",
    "    print('numeric columns found:', num_cols)\n",
    "    num_data = data[num_cols].copy()\n",
    "    # impute numeric data\n",
    "    num_data = num_data.fillna(0)\n",
    "    num_label = None\n",
    "    if labelled:\n",
    "        print('working on labels...')\n",
    "        num_label = mappedlabels.copy()\n",
    "    print(\"done transforming numeric data.\")\n",
    "    return num_data, num_label\n",
    "def clean_cat(data, labelled=False):\n",
    "    # get cat cols\n",
    "    cat_cols = [x for x in data.columns if x.endswith('_cat')]\n",
    "    if len(cat_cols) == 0:\n",
    "        print('no categorical columns found.')\n",
    "        return pd.DataFrame(), None\n",
    "    print('categorical columns found:', cat_cols)\n",
    "    cat_data = data[cat_cols].copy()\n",
    "    # impute categorical data\n",
    "    cat_data = cat_data.fillna('unknown')\n",
    "    cat_data_out = pd.get_dummies(cat_data)\n",
    "    cat_label = None\n",
    "    if labelled:\n",
    "        print('working on labels...')\n",
    "        cat_label = mappedlabels.copy()\n",
    "    print(\"done transforming categorical data.\")\n",
    "    return cat_data_out, cat_label\n",
    "def clean_image(data, labelled=False):\n",
    "    import cv2\n",
    "    # normalize image\n",
    "    def normalizeimg(img):\n",
    "        img = img.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "        rgb_mean = nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1))\n",
    "        rgb_std = nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1))\n",
    "        return (img.astype('float32') / 255 - rgb_mean) / rgb_std\n",
    "    def cleanimg(img_bytes):\n",
    "        if img_bytes == '':\n",
    "            return nd.array([])#normalizeimg(mx.nd.array([[[0]*3]*224]*244))\n",
    "        img = mx.image.imdecode(base64.b64decode(img_bytes))\n",
    "        img = mx.image.resize_short(img, 256)\n",
    "        img, _ = mx.image.center_crop(img, (224, 224))\n",
    "        return normalizeimg(img)\n",
    "    # get pretrained resnet50\n",
    "    def getresnet():\n",
    "        net = gluon.model_zoo.vision.resnet50_v1(pretrained=True, ctx=contexts)\n",
    "        print('image pre-model created...')\n",
    "        return net\n",
    "    # get cat cols\n",
    "    img_cols = [x for x in data.columns if x.endswith('_image')]\n",
    "    if len(img_cols) == 0:\n",
    "        print('no image columns found.')\n",
    "        return pd.DataFrame(), None\n",
    "    print('image columns found:', img_cols)\n",
    "    img_data = data[img_cols].copy()\n",
    "    # impute categorical data\n",
    "    img_data = img_data.fillna('')\n",
    "    # initialize model to transform images to resnet outputs\n",
    "    img_model = getresnet()\n",
    "    # create suggested img labels\n",
    "    badimg_pred = img_model(normalizeimg(mx.nd.array([[[0]*3]*224]*244)))\n",
    "    def get_pred(x):\n",
    "        if len(x) == 0:\n",
    "            return badimg_pred\n",
    "        else:\n",
    "            return img_model(x)\n",
    "    map2bads =  []\n",
    "    for col in img_cols:\n",
    "        print('cleaning:', col)\n",
    "        map2bads.append(img_data[col] == '')\n",
    "        img_data[col] = img_data[col].parallel_apply(cleanimg)\n",
    "        print('images cleaned. now pre-predicting...')\n",
    "        img_data[col] = img_data[col].parallel_apply(get_pred)\n",
    "    image_label = []\n",
    "    if labelled:\n",
    "        print('working on labels...')\n",
    "        import functools\n",
    "        mlabs = mappedlabels.copy()\n",
    "        for i in range(len(map2bads)):\n",
    "            image_label.append(mlabs.copy())\n",
    "            image_label[i][map2bads[i]] = numcats\n",
    "    print(\"done transforming image data.\")\n",
    "    return img_data, image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_data, text_label = clean_text(dataset, labelled=True)\n",
    "num_data, num_label = clean_num(dataset, labelled=True)\n",
    "cat_data, cat_label = clean_cat(dataset, labelled=True)\n",
    "img_data, img_label = clean_image(dataset, labelled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model-specific values to pickle file for inference container\n",
    "has_text = (len(text_data) != 0)\n",
    "has_num = (len(num_data) != 0)\n",
    "has_cat = (len(cat_data) != 0)\n",
    "has_img = (len(img_data) != 0)\n",
    "model_config = {\n",
    "    \"columns\": dataset.columns.tolist(),\n",
    "    \"columns_text\": text_data.columns.tolist(),\n",
    "    \"columns_num\": num_data.columns.tolist(),\n",
    "    \"columns_cat\": cat_data.columns.tolist(),\n",
    "    \"columns_img\": img_data.columns.tolist(),\n",
    "    \"has_text\": has_text,\n",
    "    \"has_num\": has_num,\n",
    "    \"has_cat\": has_cat,\n",
    "    \"has_img\": has_img,\n",
    "    \"catmap\": catmap,\n",
    "    \"invcatmap\": invcatmap,\n",
    "    \"numcats\": numcats,\n",
    "    \"cats\": cats,\n",
    "    \"columns_cat_dummies\": cat_data.columns.tolist(),\n",
    "}\n",
    "with open(model_path+'model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(model_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phase one - per-modality training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon import TabularPrediction as task\n",
    "\n",
    "if has_text:\n",
    "    print('loading text data')\n",
    "    agluon_text_train_data = task.Dataset(pd.concat([text_data, pd.Series(text_label, name='label')], axis=1))\n",
    "if has_num:\n",
    "    print('loading num data')\n",
    "    agluon_num_train_data = task.Dataset(pd.concat([num_data, pd.Series(num_label, name='label')], axis=1))\n",
    "if has_cat:\n",
    "    print('loading cat data')\n",
    "    agluon_cat_train_data = task.Dataset(pd.concat([cat_data, pd.Series(cat_label, name='label')], axis=1))\n",
    "if has_img:\n",
    "    print('loading img data')\n",
    "    agluon_img_train_data = []\n",
    "    for i in range(len(img_data.columns)):\n",
    "        from itertools import zip_longest\n",
    "        curr_img_feature = pd.DataFrame.from_records(zip_longest(\n",
    "            *img_data.iloc[:, i].parallel_apply(lambda x: x[0].asnumpy()).values)).transpose()\n",
    "        agluon_img_train_data.append(task.Dataset(\n",
    "            pd.concat([curr_img_feature, pd.Series(img_label[i], name='label')], axis=1)))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train on each existing modality\n",
    "if has_text:\n",
    "    predictor_text = task.fit(\n",
    "        train_data=agluon_text_train_data,\n",
    "        label=target_column,\n",
    "        eval_metric=eval_metric,\n",
    "        #presets='best_quality',\n",
    "        time_limits=time_limits,\n",
    "        id_columns=[id_column],\n",
    "        ngpus_per_trial=8,\n",
    "        nthreads_per_trial=os.cpu_count(),\n",
    "        verbosity=3,\n",
    "        problem_type='multiclass',\n",
    "        output_directory=model_path+'text/'\n",
    "    )\n",
    "    print('done training text')\n",
    "if has_num:\n",
    "    predictor_num = task.fit(\n",
    "        train_data=agluon_num_train_data,\n",
    "        label=target_column,\n",
    "        eval_metric=eval_metric,\n",
    "        #presets='best_quality',\n",
    "        time_limits=time_limits,\n",
    "        id_columns=[id_column],\n",
    "        ngpus_per_trial=8,\n",
    "        nthreads_per_trial=os.cpu_count(),\n",
    "        verbosity=3,\n",
    "        problem_type='multiclass',\n",
    "        output_directory=model_path+'num/'\n",
    "    )\n",
    "    print('done training num')\n",
    "if has_cat:\n",
    "    predictor_cat = task.fit(\n",
    "        train_data=agluon_cat_train_data,\n",
    "        label=target_column,\n",
    "        eval_metric=eval_metric,\n",
    "        #presets='best_quality',\n",
    "        time_limits=time_limits,\n",
    "        id_columns=[id_column],\n",
    "        ngpus_per_trial=8,\n",
    "        nthreads_per_trial=os.cpu_count(),\n",
    "        verbosity=3,\n",
    "        problem_type='multiclass',\n",
    "        output_directory=model_path+'cat/'\n",
    "    )\n",
    "    print('done training cat')\n",
    "if has_img:\n",
    "    predictor_img = []\n",
    "    for i in range(len(agluon_img_train_data)):\n",
    "        task.fit(\n",
    "            train_data=agluon_img_train_data[i],\n",
    "            label=target_column,\n",
    "            eval_metric=eval_metric,\n",
    "            #presets='best_quality',\n",
    "            time_limits=time_limits,\n",
    "            id_columns=[id_column],\n",
    "            ngpus_per_trial=8,\n",
    "            nthreads_per_trial=os.cpu_count(),\n",
    "            verbosity=3,\n",
    "            problem_type='multiclass',\n",
    "            output_directory=model_path+'img/'+img_data.columns[i]\n",
    "        )\n",
    "    print('done training img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on each existing modality\n",
    "if has_text:\n",
    "    predictor_text = task.load(model_path+'text/')\n",
    "    print('done loading text model')\n",
    "if has_num:\n",
    "    predictor_num = task.load(model_path+'num/')\n",
    "    print('done loading num model')\n",
    "if has_cat:\n",
    "    predictor_cat = task.load(model_path+'cat/')\n",
    "    print('done loading cat model')\n",
    "if has_img:\n",
    "    predictor_img = task.load(model_path+'img/')\n",
    "    print('done loading img model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_text:\n",
    "    preds_text = predictor_text.predict_proba(agluon_text_train_data.drop(columns=['label']))\n",
    "    print('done generating text unimodal preds')\n",
    "if has_num:\n",
    "    preds_num = predictor_num.predict_proba(agluon_num_train_data.drop(columns=['label']))\n",
    "    print('done generating num unimodal preds')\n",
    "if has_cat:\n",
    "    preds_cat = predictor_cat.predict_proba(agluon_cat_train_data.drop(columns=['label']))\n",
    "    print('done generating cat unimodal preds')\n",
    "if has_img:\n",
    "    preds_img = []\n",
    "    for i in range(len(predictor_img)):\n",
    "        preds_img.append(predictor_img[i].predict_proba(agluon_img_train_data[i].drop(columns=['label'])))\n",
    "    print('done generating img unimodal preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phase 2 - wholistic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "wholistic_train = pd.DataFrame()\n",
    "if has_text:\n",
    "    wholistic_train = pd.concat([wholistic_train, pd.DataFrame(preds_text)], axis=1)\n",
    "if has_num:\n",
    "    wholistic_train = pd.concat([wholistic_train, pd.DataFrame(preds_num)], axis=1)\n",
    "if has_cat:\n",
    "    wholistic_train = pd.concat([wholistic_train, pd.DataFrame(preds_cat)], axis=1)\n",
    "if has_img:\n",
    "    wholistic_train = pd.concat([wholistic_train, pd.DataFrame(preds_img)], axis=1)\n",
    "wholistic_input_size = wholistic_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(\n",
    "        int(wholistic_input_size*float(2/3))+numcats,\n",
    "        input_shape=(wholistic_input_size,),\n",
    "        activation='relu')\n",
    "    )\n",
    "    model.add(layers.Dense(int(wholistic_input_size*float(1/3))+numcats, activation='relu'))\n",
    "    model.add(layers.Dense(numcats, activation='sigmoid'))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    wholistic_train.values,\n",
    "    mappedlabels.values,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'wholistic/wholistic_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}